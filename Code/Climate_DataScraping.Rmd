---
title: 'Blue Mesa: Data Scraping'
author: "Stephanie Kinser"
date: '2022-03-29'
output: pdf_document
---

```{r setup, include=FALSE}
getwd()

library(tidyverse)
library(dplyr)
#install.packages("lubridate")
library(lubridate)
#install.packages("rvest")
library(rvest)

```

```{r}
#set scraping website
webpage <- read_html('https://climate.colostate.edu/data_access.html')

webpage
```

```{r}
#scrape variables
Date <- webpage %>% html_nodes('td:nth-child(1)') %>% html_text()

MaxT <- webpage %>% html_nodes('td:nth-child(2)') %>% html_text()

MinT <- webpage %>% html_nodes('td:nth-child(3)') %>% html_text()

Precip <- webpage %>% html_nodes('td:nth-child(4)') %>% html_text()

Snow <- webpage %>% html_nodes('td:nth-child(5)') %>% html_text()
```

```{r}

climate_df <- data.frame(as.Date(Date, format = "%Y-%m"), MaxT, MinT, Precip, Snow)
```

>Methods for data scraping: #when pulling data, base on power plant limited data (res/platn), removed footnotes and changed column names ; crystal might be okay
#google crystal reservoir in google maps , see if you can find a nearby town that matches climate data, montrose was close to crystal
#blue mesa lake, copy and paste table into excel and save as csv, or try magnifying glass 
